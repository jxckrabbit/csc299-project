For the overall process of creating my final project I decided that I just wanted to use copilot and pytest, as while spec-kit was interesting it was highly complicated and felt limiting in the sense that I had to follow exactly their process. I knew that I wanted to have unit tests for it, pytest seemed easiest, because using code that isn't adequately tested is not a smart decision. I had gained a few ideas throughout the quarter of interesting features I could add, but the truest "unique" feature (my recommendation system with "styles") came from moving in with my parents for this quarter. Being home again meant that cleaning the house was once again a family endeavour, and my parents still had the exact same list of tasks in the exact same order as they used to be. I originally wanted to see if I could create some kind of dependency system, as in you can only complete certain tasks when others were done, but that didn't seem widely applicable and I figured it would be difficult to succinctly explain that to an AI. As a result, I stuck to the simpler idea of letting a user decide how they want to complete tasks; all of one type at once or one of every type. This was inspired by how I prefer to complete my homework, one class at a time instead of mixing classes in the same day. 

The implementation process for my idea wasn't without it's hitches. I had to create an entire other repository for the project as shared basic file names and the previous prototypes' data kept messing with the AI agent and making it put new things in old prototypes, even when I specified to create anything in the new "final-project" directory. I added the basics first, like the user system and the task categorization. Then I went to adding my recommend feature, but realized that I needed to keep track of whatever task was "current" for that system, since it relied on a base category to pick either the same or different of. Along this process, the AI suggested many things but one I actually thought was a good idea was listing tasks by category, so I allowed it to add that feature. 

I knew I had to add something that would separately call an AI, and just used the openai api key that we had set up previously. The process for this was...interesting to say the least, as despite being an AI itself, copilot really didn't want to run the AI integrated function test with real AI. I assume this is a "don't share your API key with an unsafe system so you can't sue us" issue, but it was vaguely annoying. Eventually I did a few tests and managed to make it read that my API key was valid and existed, and pass a test generation prompt into it. 